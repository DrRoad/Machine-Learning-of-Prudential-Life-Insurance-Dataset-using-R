install.packages('ISLR')
library(ISLR)
print(head(College,2))
maxs <- apply(College[,2:18], 2, max)
mins <- apply(College[,2:18], 2, min)
scaled.data <- as.data.frame(scale(College[,2:18],center = mins, scale = maxs - mins))
print(head(scaled.data,2))
Private = as.numeric(College$Private)-1
library(neuralnet)
data = cbind(Private,scaled.data)
library(caTools)
set.seed(101)
split = sample.split(data$Private, SplitRatio = 0.70)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
feats <- names(scaled.data)
f <- paste(feats,collapse=' + ')
f <- paste('Private ~',f)
f <- as.formula(f)
f
library(neuralnet)
nn <- neuralnet(f,data,hidden=c(10,10,10),linear.output=FALSE)
install.packages('neuralnet')
library(neuralnet)
nn <- neuralnet(f,data,hidden=c(10,10,10),linear.output=FALSE)
predicted.nn.values <- compute(nn,test[2:18])
print(head(predicted.nn.values$net.result))
predicted.nn.values$net.result <- sapply(predicted.nn.values$net.result,round,digits=0)
table(test$Private,predicted.nn.values$net.result)
library("NLP")
install.packages("NLP")
library("NLP")
library("openNLP")
install.packages("openNLP")
library("NLP")
library("openNLP")
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov, 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."), collapse = "")
s <- as.String(s)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
post_tag_annotator <- Maxent_POS_Tag_Annotator()
post_tag_annotator
a3 <- annotate(s, post_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
post_tag_annotator <- Maxent_POS_Tag_Annotator()
post_tag_annotator
a3 <- annotate(s, post_tag_annotator, a2)
a3
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
tags
table(tags)
sprintf("%s/%s", s[a3w], tags)
a3ws2 <- annotations_in_spans(subset(a3, type == word"")
subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == "word")subset(a3, type == "sentence")[2L])[[1L]]
a3ws2 <- annotations_in_spans(subset(a3, type == "word"),
subset(a3, type == "sentence")[2L])[[1L]]
sprintf("%s/%s", s[a3w3s], sapply(a3ws2$features, `[[`, "POS"))
sprintf("%s/%s", s[a3ws2], sapply(a3ws2$features, `[[`, "POS"))
library(XLConnect)
library("neuralnet")
loan=read.xlsx("loan.xlsx",sheetIndex = 1,HEADER=TRUE)
library("XLConnect")
install.packages("rvest")
library(rvest)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
View(all_reviews)
install.packages("rJava")
library(rJava)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
library(NLP)
library(openNLP)
library(RWeka)
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
library(stringi)
library(stringi)
library(SnowballC)
install.packages("SnowballC")
library(stringi)
library(SnowballC)
library(Rcpp)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
install.packages("Rstem")
install.packages("tm")
install.packages("purrr")
library(stringi)
library(Rstem)
library(tm)
library(purrr)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
library(dplyr)
song <- paste0("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
tokenize_words(song)
Token_Tokenizer(song)
song1 <- Token_Tokenizer(song)
View(song1)
song <- paste("How many roads must a man walk down\n",
"Before you call him a man?\n",
"How many seas must a white dove sail\n",
"Before she sleeps in the sand?\n",
"\n",
"How many times must the cannonballs fly\n",
"Before they're forever banned?\n",
"The answer, my friend, is blowin' in the wind.\n",
"The answer is blowin' in the wind.\n")
songtokenize_words(song)
song
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
token(all_reviews)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
View(all_reviews)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
library(rvest)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
install.packages("RTextTools")
library(RTextTools)
pages <- 100
all_reviews <- NULL
View(all_reviews)
pages <- 10
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
wordStem(tokenized_reviews)
library(deepnet)
install.packages("deepnet")
library(deepnet)
show.digit <- function(arr784, col=gray(12:1/12), ...) {
image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}
pages <- 5
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
library(tm)
library(RTextTools)
library(rvest)
library(dplyr)
library(plyr)
library(stringr)
library(e1071)
library(forecast)
options(max.print=1000000)
pages <- 5
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
iniReviews <- all_reviews
all_reviews <- iniReviews
all_reviews <- gsub("/", "", all_reviews)
all_reviews <- gsub("'\'", "", all_reviews)
all_reviews <- gsub("[\r\n]", " ", all_reviews)
all_reviews <- gsub("u0096", "", all_reviews)
all_reviews <- gsub("[[:punct:]]", "", all_reviews)
all_reviews <- gsub('\\d+', '', all_reviews)
all_reviews <-gsub('\\p{So}|\\p{Cn}', '', all_reviews, perl = TRUE)
myCorpus <- Corpus(VectorSource(all_reviews))
inspect(myCorpus)
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus,stripWhitespace)
myCorpus <- tm_map(myCorpus,PlainTextDocument)
myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus,removeWords,stopwords("english"))
myCorpus <- tm_map(myCorpus,stripWhitespace)
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
poslist <- read.delim(file="D:/Spring 2017/ADS/Final Project/positive-words.txt", header=FALSE, stringsAsFactors=FALSE)
View(poslist)
names(poslist) <- c('posword')
poslist <- wordStem(as.matrix(poslist))
neglist <- read.delim(file="D:/Spring 2017/ADS/Final Project/negative-words.txt", header=FALSE, stringsAsFactors=FALSE)
names(neglist) <- c('negword')
neglist <- wordStem(as.matrix(neglist))
sentimentAnalysis <- function(all_reviews, neglist, poslist){
result <- matrix('', 0, 3)
reviews_scores <- laply(all_reviews, function(review, neglist, poslist){
#preserving review text
initial_review <- review
#creating wordlist of review
tokenized_review <- token(review)
#stemming review list
stemmed_review <- wordStem(tokenized_review)
#print(stemmed_review)
#Cleaning the stemmed text
stemmed_review <- gsub('[[:punct:]]', '', stemmed_review )
#Removing Control Characters
stemmed_review  <- gsub('[[:cntrl:]]', '', stemmed_review)
#List with matches between review words and each category(positive & negative)
positiveMatch_list <- match(stemmed_review, poslist)
negativeMatch_list <- match(stemmed_review, neglist)
#calculate sum of number of words in each matched category
positiveMatch_list <- sum(!is.na(positiveMatch_list))
negativeMatch_list <- sum(!is.na(negativeMatch_list))
#total <- positiveMatch_list + negativeMatch_list
#posRatio <- (positiveMatch_list/total)
#negRatio <- (negativeMatch_list/total)
score <- c(positiveMatch_list, negativeMatch_list)
#add row to scores table
newrow <- c(initial_review, score)
result  <- rbind(result, newrow)
return(result)
}, neglist, poslist)
return(reviews_scores)
}
fin_Result <- as.data.frame(sentimentAnalysis(myCorpus, neglist, poslist))
colnames(fin_Result) <- c('Review', 'posRatio', 'negRatio')
View(fin_Result)
fin_Result[,1] <- as.character( fin_Result[, 1] )
fin_Result[,2] <- as.numeric(as.character( fin_Result[, 2] ))
fin_Result[,3] <- as.numeric(as.character( fin_Result[, 3] ))
fin_Result["Result"] <-NA
for(i in 1:dim(fin_Result)[1]){
if(fin_Result[i,2]>fin_Result[i,3]) {
fin_Result[i,4]<-"positive"
}
else{
fin_Result[i,4]<-"negative"
}
}
corpus_clean <- tm_map(myCorpus, stemDocument,language="english")
inspect(corpus_clean)
corpus_dtm <- DocumentTermMatrix(corpus_clean)
inspect(corpus_dtm)
write.csv(cbind(as.matrix(corpus_dtm),fin_Result$Result),"D:/Spring 2017/ADS/Final Project/final.csv")
getwd()
setwd("D:/Spring 2017/ADS/Final Project")
final <-read.csv(file="final.csv", header=TRUE, sep=",")
train<-final[1:40,]
test<-final[41:50,]
ncol(final)
model1 = svm(RESULT ~ ., kernel = "polynomial", cost =0.01 ,gamma=0.1, data = train, scale = F)
train<-final[1:15,]
test<-final[16:21,]
model1 = svm(RESULT ~ ., kernel = "polynomial", cost =0.01 ,gamma=0.1, data = train, scale = F)
final <-read.csv(file="final.csv", header=TRUE, sep=",")
View(final)
train<-final[1:15,]
test<-final[16:21,]
obj<-tune(svm, RESULT~.,kernel ="radial", data= train, ranges=list(gamma=10^(-2:2), cost=10^(-2:2)))
predictions <-  predict(model1, test)
model1 = svm(RESULT ~ ., kernel = "polynomial", cost =0.01 ,gamma=0.1, data = train, scale = F)
predictions <-  predict(model1, test)
accuracy(predictions,test$RESULT)
library(forecast)
accuracy(predictions,test$RESULT)
library(ade4)
accuracy(predictions,test$RESULT)
library(ggplot2)
library("rpart")
library("rpart.plot")
library(e1071)
library(forecast)
library(ade4)
library(ggplot2)
predictions <-  predict(model1, test)
accuracy(predictions,test$RESULT)
install.packages("forecast")
install.packages("forecast")
install.packages("ade4")
install.packages("ade4")
install.packages("forecastHybrid")
library(dplyr)
library(plyr)
library(stringr)
library(e1071)
library(forecast)
library(ade4)
library(ggplot2)
library("rpart")
library("rpart.plot")
accuracy(predictions,test$RESULT)
predictions <-  predict(model1, test)
accuracy(predictions,test$RESULT)
class(predictions)
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
sample_index = sample(1000, 200)
test_dataset = dataset[sample_index,]
train_dataset = dataset[-sample_index,]
model = svm(Class ~ ., kernel = "radial", cost = 10, gamma = 0.1, data = train_dataset, scale = F)
predictions <-  predict(model, test_dataset[-10])
table(test_dateset[,10], predictions)
ncol(final)
table(test[,550], predictions)
table(test[,550], predictions)
accuracy(predictions,test$RESULT)
model1 = svm(RESULT ~ .,type='C-classification', kernel = "polynomial", cost =0.01 ,gamma=0.1, data = train, scale = F)
predictions <-  predict(model1, test)
class(predictions)
table(test[,550], predictions)
accuracy(predictions,test$RESULT)
library(forecast)
library(ade4)
library(e1071)
library(ggplot2)
library("rpart")
library("rpart.plot")
setwd("D:/Spring 2017/ADS/Final Project")
setwd("D:/Spring 2017/ADS/MIdterm Project")
mydata <- read.csv(file="train.csv", header=TRUE, sep=",")
options(max.print=1000000)
mydata$Family_Hist_3 <- NULL
mydata$Family_Hist_5 <- NULL
mydata$Medical_History_10 <- NULL
mydata$Medical_History_15 <- NULL
mydata$Medical_History_24 <- NULL
mydata$Medical_History_32 <- NULL
mydata$Id <- NULL
mylist <- c(unlist(lapply(mydata, function(x) any(is.na(x)))))
mydata$Employment_Info_1[is.na(mydata$Employment_Info_1)] <- mean(mydata$Employment_Info_1, na.rm = T)
mydata$Employment_Info_4[is.na(mydata$Employment_Info_4)] <- mean(mydata$Employment_Info_4, na.rm = T)
mydata$Employment_Info_6[is.na(mydata$Employment_Info_6)] <- mean(mydata$Employment_Info_6, na.rm = T)
mydata$Insurance_History_5[is.na(mydata$Insurance_History_5)] <- mean(mydata$Insurance_History_5, na.rm = T)
mydata$Family_Hist_2[is.na(mydata$Family_Hist_2)] <- mean(mydata$Family_Hist_2, na.rm = T)
mydata$Family_Hist_4[is.na(mydata$Family_Hist_4)] <- mean(mydata$Family_Hist_4, na.rm = T)
mydata$Medical_History_1[is.na(mydata$Medical_History_1)] <- mean(mydata$Medical_History_1, na.rm = T)
data_ctgr <- mydata[c("Medical_History_1","Product_Info_1", "Product_Info_2", "Product_Info_3", "Product_Info_5", "Product_Info_6", "Product_Info_7", "Employment_Info_2", "Employment_Info_3", "Employment_Info_5", "InsuredInfo_1", "InsuredInfo_2", "InsuredInfo_3", "InsuredInfo_4", "InsuredInfo_5", "InsuredInfo_6", "InsuredInfo_7", "Insurance_History_1", "Insurance_History_2", "Insurance_History_3", "Insurance_History_4", "Insurance_History_7", "Insurance_History_8", "Insurance_History_9", "Family_Hist_1", "Medical_History_2", "Medical_History_3", "Medical_History_4", "Medical_History_5", "Medical_History_6", "Medical_History_7", "Medical_History_8", "Medical_History_9", "Medical_History_11", "Medical_History_12", "Medical_History_13", "Medical_History_14", "Medical_History_16", "Medical_History_17", "Medical_History_18", "Medical_History_19", "Medical_History_20", "Medical_History_21", "Medical_History_22", "Medical_History_23", "Medical_History_25", "Medical_History_26", "Medical_History_27", "Medical_History_28", "Medical_History_29", "Medical_History_30", "Medical_History_31", "Medical_History_33", "Medical_History_34", "Medical_History_35", "Medical_History_36", "Medical_History_37", "Medical_History_38", "Medical_History_39", "Medical_History_40", "Medical_History_41")]
OneToCconv <- acm.disjonctif(data_ctgr)
data_cntg <- mydata[c("Product_Info_4", "Ins_Age", "Ht", "Wt", "BMI", "Employment_Info_1", "Employment_Info_4", "Employment_Info_6", "Insurance_History_5", "Family_Hist_2", "Family_Hist_4")]
data_dummy<-mydata[c("Medical_Keyword_1","Medical_Keyword_2","Medical_Keyword_3","Medical_Keyword_4","Medical_Keyword_5","Medical_Keyword_6","Medical_Keyword_7","Medical_Keyword_8","Medical_Keyword_9","Medical_Keyword_10","Medical_Keyword_11","Medical_Keyword_12","Medical_Keyword_13","Medical_Keyword_14","Medical_Keyword_15","Medical_Keyword_16","Medical_Keyword_17","Medical_Keyword_18","Medical_Keyword_19", "Medical_Keyword_20", "Medical_Keyword_21", "Medical_Keyword_22", "Medical_Keyword_23","Medical_Keyword_24", "Medical_Keyword_25", "Medical_Keyword_26", "Medical_Keyword_27", "Medical_Keyword_28", "Medical_Keyword_29","Medical_Keyword_30", "Medical_Keyword_31", "Medical_Keyword_32", "Medical_Keyword_33","Medical_Keyword_34", "Medical_Keyword_35","Medical_Keyword_36", "Medical_Keyword_37", "Medical_Keyword_38", "Medical_Keyword_39", "Medical_Keyword_40", "Medical_Keyword_41", "Medical_Keyword_42", "Medical_Keyword_43", "Medical_Keyword_44", "Medical_Keyword_45","Medical_Keyword_46", "Medical_Keyword_47","Medical_Keyword_48")]
final_data <- data.frame(c(OneToCconv, data_cntg,data_dummy))
final_pca_data <- prcomp(final_data)
summary(final_pca_data)
final_pca_data_trimmed <- data.frame(final_pca_data$x[,1:116])
pcaYVar <- data.frame(c(final_pca_data_trimmed, original_data[c("Response")]))
pcaYVar$Response<-as.numeric(pcaYVar$Response)
pcaYVar <- data.frame(c(final_pca_data_trimmed, mydata[c("Response")]))
pcaYVar$Response<-as.numeric(pcaYVar$Response)
train <-pcaYVar[1:49382,]
test <- pcaYVar[49383:59382,]
---------------------------------------------------------------------------------------------
#SVM Regression
#Calculating PCA for final consolidated data.
final_pca_data <- prcomp(final_data)
---------------------------------------------------------------------------------------------
#SVM Regression
#Calculating PCA for final consolidated data.
final_pca_data <- prcomp(final_data)
test <- pcaYVar[49383:59382,]
---------------------------------------------------------------------------------------------
#SVM Regression
#Calculating PCA for final consolidated data.
final_pca_data <- prcomp(final_data)
